{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acaa45b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3be605",
   "metadata": {},
   "source": [
    "# Step 1. 정제, 정규화, 전처리 (영어, 프랑스어 모두!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ca02e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080bd0cd",
   "metadata": {},
   "source": [
    "데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89df45e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 197463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48168</th>\n",
       "      <td>Do you play in a band?</td>\n",
       "      <td>Joues-tu dans une fanfare ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69305</th>\n",
       "      <td>Your house is fantastic.</td>\n",
       "      <td>Votre maison est fantastique.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133555</th>\n",
       "      <td>Why don't you take your coat off?</td>\n",
       "      <td>Pourquoi n'ôtez-vous pas votre manteau ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164128</th>\n",
       "      <td>We could see enemy ships on the horizon.</td>\n",
       "      <td>Nous pouvions voir des navires ennemis à l'hor...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139205</th>\n",
       "      <td>You expect too much of your child.</td>\n",
       "      <td>Tu en demandes trop à ton enfant.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             eng  \\\n",
       "48168                     Do you play in a band?   \n",
       "69305                   Your house is fantastic.   \n",
       "133555         Why don't you take your coat off?   \n",
       "164128  We could see enemy ships on the horizon.   \n",
       "139205        You expect too much of your child.   \n",
       "\n",
       "                                                      fra  \\\n",
       "48168                         Joues-tu dans une fanfare ?   \n",
       "69305                       Votre maison est fantastique.   \n",
       "133555           Pourquoi n'ôtez-vous pas votre manteau ?   \n",
       "164128  Nous pouvions voir des navires ennemis à l'hor...   \n",
       "139205                  Tu en demandes trop à ton enfant.   \n",
       "\n",
       "                                                       cc  \n",
       "48168   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "69305   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "133555  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "164128  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "139205  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b06cfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197458</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197459</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197460</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197461</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197462</th>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197463 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "0                                                     Go.   \n",
       "1                                                     Go.   \n",
       "2                                                     Go.   \n",
       "3                                                     Go.   \n",
       "4                                                     Hi.   \n",
       "...                                                   ...   \n",
       "197458  A carbon footprint is the amount of carbon dio...   \n",
       "197459  Death is something that we're often discourage...   \n",
       "197460  Since there are usually multiple websites on a...   \n",
       "197461  If someone who doesn't know your background sa...   \n",
       "197462  It may be impossible to get a completely error...   \n",
       "\n",
       "                                                      fra  \\\n",
       "0                                                    Va !   \n",
       "1                                                 Marche.   \n",
       "2                                              En route !   \n",
       "3                                                 Bouge !   \n",
       "4                                                 Salut !   \n",
       "...                                                   ...   \n",
       "197458  Une empreinte carbone est la somme de pollutio...   \n",
       "197459  La mort est une chose qu'on nous décourage sou...   \n",
       "197460  Puisqu'il y a de multiples sites web sur chaqu...   \n",
       "197461  Si quelqu'un qui ne connaît pas vos antécédent...   \n",
       "197462  Il est peut-être impossible d'obtenir un Corpu...   \n",
       "\n",
       "                                                       cc  \n",
       "0       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "1       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "2       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "3       CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "4       CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "...                                                   ...  \n",
       "197458  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "197459  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "197460  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "197461  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "197462  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "\n",
       "[197463 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d146dc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3834</th>\n",
       "      <td>Take a bite.</td>\n",
       "      <td>Prends une bouchée !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30710</th>\n",
       "      <td>I'm still in shock.</td>\n",
       "      <td>Je suis encore sous le choc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13188</th>\n",
       "      <td>Be more precise.</td>\n",
       "      <td>Soit plus précis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10230</th>\n",
       "      <td>Here is my key.</td>\n",
       "      <td>Voici ma clef.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21766</th>\n",
       "      <td>What a nice bike!</td>\n",
       "      <td>Quel beau vélo !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       eng                           fra\n",
       "3834          Take a bite.          Prends une bouchée !\n",
       "30710  I'm still in shock.  Je suis encore sous le choc.\n",
       "13188     Be more precise.             Soit plus précis.\n",
       "10230      Here is my key.                Voici ma clef.\n",
       "21766    What a nice bike!              Quel beau vélo !"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6670a160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4155</th>\n",
       "      <td>We love you.</td>\n",
       "      <td>\\t Nous vous aimons. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14907</th>\n",
       "      <td>I've got a plan.</td>\n",
       "      <td>\\t Je dispose d'un plan. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>Tom is back.</td>\n",
       "      <td>\\t Tom est de retour. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8518</th>\n",
       "      <td>Take a number.</td>\n",
       "      <td>\\t Prends un numéro ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3854</th>\n",
       "      <td>Talk slowly.</td>\n",
       "      <td>\\t Parlez lentement. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    eng                          fra\n",
       "4155       We love you.      \\t Nous vous aimons. \\n\n",
       "14907  I've got a plan.  \\t Je dispose d'un plan. \\n\n",
       "3964       Tom is back.     \\t Tom est de retour. \\n\n",
       "8518     Take a number.     \\t Prends un numéro ! \\n\n",
       "3854       Talk slowly.      \\t Parlez lentement. \\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c0690e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 4, 7], [19, 4, 7], [19, 4, 7]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46eb64f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 1, 19, 5, 1, 31, 1, 10],\n",
       " [9, 1, 15, 5, 12, 16, 28, 2, 13, 1, 10],\n",
       " [9, 1, 2, 7, 1, 12, 8, 11, 4, 2, 1, 31, 1, 10]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dcd582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 51\n",
      "프랑스어 단어장의 크기 : 73\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb95d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 19\n",
      "프랑스어 시퀀스의 최대 길이 61\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c0d5a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 51\n",
      "프랑스어 단어장의 크기 : 73\n",
      "영어 시퀀스의 최대 길이 19\n",
      "프랑스어 시퀀스의 최대 길이 61\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5083204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12f6b355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 1, 19, 5, 1, 31, 1], [9, 1, 15, 5, 12, 16, 28, 2, 13, 1], [9, 1, 2, 7, 1, 12, 8, 11, 4, 2, 1, 31, 1]]\n",
      "[[1, 19, 5, 1, 31, 1, 10], [1, 15, 5, 12, 16, 28, 2, 13, 1, 10], [1, 2, 7, 1, 12, 8, 11, 4, 2, 1, 31, 1, 10]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29693ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 19)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 61)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 61)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b41a2491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  4  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f166fd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 19, 51)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 61, 73)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 61, 73)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2727385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (33000, 19, 51)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (33000, 61, 73)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (33000, 61, 73)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16f70078",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:30000]\n",
    "decoder_input_train = decoder_input[:30000]\n",
    "decoder_target_train = decoder_target[:30000]\n",
    "\n",
    "encoder_input_test = encoder_input[30000:]\n",
    "decoder_input_test = decoder_input[30000:]\n",
    "decoder_target_test = decoder_target[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26e97848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 19, 51)\n",
      "(30000, 61, 73)\n",
      "(30000, 61, 73)\n",
      "(3000, 19, 51)\n",
      "(3000, 61, 73)\n",
      "(3000, 61, 73)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_train.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_target_train.shape)\n",
    "print(encoder_input_test.shape)\n",
    "print(decoder_input_test.shape)\n",
    "print(decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeaff6e",
   "metadata": {},
   "source": [
    "모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc113b1e",
   "metadata": {},
   "source": [
    "필요한 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c53b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "922c3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM의 출력 차원\n",
    "latent_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd8ff467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "# 디코더로 전달할 hidden state, cell state를 리턴. encoder_outputs은 여기서는 불필요.\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72677d9",
   "metadata": {},
   "source": [
    "디코더를 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e196fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf9f4b",
   "metadata": {},
   "source": [
    "디코더의 출력층 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf5b09fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fe35b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 51)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 315392      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 672,073\n",
      "Trainable params: 672,073\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b261095b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 30s 19ms/step - loss: 1.1394 - val_loss: 0.9600\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.7054 - val_loss: 0.7626\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.5926 - val_loss: 0.6826\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.5257 - val_loss: 0.6337\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.4763 - val_loss: 0.5891\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.4401 - val_loss: 0.5501\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.4113 - val_loss: 0.5286\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.3881 - val_loss: 0.5053\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.3686 - val_loss: 0.4856\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.3523 - val_loss: 0.4736\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.3381 - val_loss: 0.4591\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.3256 - val_loss: 0.4493\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.3143 - val_loss: 0.4492\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.3043 - val_loss: 0.4399\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.2952 - val_loss: 0.4345\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.2867 - val_loss: 0.4251\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.2789 - val_loss: 0.4295\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.2717 - val_loss: 0.4231\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.2648 - val_loss: 0.4271\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.2582 - val_loss: 0.4197\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.2522 - val_loss: 0.4253\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.2464 - val_loss: 0.4192\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.2410 - val_loss: 0.4177\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.2356 - val_loss: 0.4160\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.2308 - val_loss: 0.4238\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.2260 - val_loss: 0.4202\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.2213 - val_loss: 0.4253\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.2168 - val_loss: 0.4270\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.2129 - val_loss: 0.4254\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.2086 - val_loss: 0.4283\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.2046 - val_loss: 0.4293\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.2011 - val_loss: 0.4310\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1975 - val_loss: 0.4313\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1938 - val_loss: 0.4366\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1905 - val_loss: 0.4385\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1871 - val_loss: 0.4416\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1839 - val_loss: 0.4435\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1808 - val_loss: 0.4434\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1776 - val_loss: 0.4488\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1751 - val_loss: 0.4500\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1721 - val_loss: 0.4547\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1692 - val_loss: 0.4615\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1667 - val_loss: 0.4606\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1639 - val_loss: 0.4651\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1615 - val_loss: 0.4668\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1590 - val_loss: 0.4707\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1567 - val_loss: 0.4712\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1545 - val_loss: 0.4768\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1521 - val_loss: 0.4770\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1497 - val_loss: 0.4848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f58f7ff5520>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384875e5",
   "metadata": {},
   "source": [
    "모델 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b30976",
   "metadata": {},
   "source": [
    "인코더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66e920f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 51)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 315392    \n",
      "=================================================================\n",
      "Total params: 315,392\n",
      "Trainable params: 315,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61deef3",
   "metadata": {},
   "source": [
    "디코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15626482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1203c2",
   "metadata": {},
   "source": [
    "디코더 출력층 재설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e95ac03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 356,681\n",
      "Trainable params: 356,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ae41afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d23df68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fffd0756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장:  Bouge ! \n",
      "번역기가 번역한 문장:  va-t’en ! \n",
      "-----------------------------------\n",
      "입력 문장: Hello!\n",
      "정답 문장:  Bonjour ! \n",
      "번역기가 번역한 문장:  bonjour ! \n",
      "-----------------------------------\n",
      "입력 문장: Got it?\n",
      "정답 문장:  T'as capté ? \n",
      "번역기가 번역한 문장:  compris ? \n",
      "-----------------------------------\n",
      "입력 문장: Hang on.\n",
      "정답 문장:  Tiens bon ! \n",
      "번역기가 번역한 문장:  tiens bon ! \n",
      "-----------------------------------\n",
      "입력 문장: Here's $5.\n",
      "정답 문장:  Voilà cinq dollars. \n",
      "번역기가 번역한 문장:  tue vous ! \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e946a",
   "metadata": {},
   "source": [
    "# 단어 Level로 번역기 업그레이드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b8d49fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0455ea6",
   "metadata": {},
   "source": [
    "시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8524ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef4d860e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 197463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42967</th>\n",
       "      <td>I saw you doing that.</td>\n",
       "      <td>Je vous ai vu faire cela.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>You can't give up like this.</td>\n",
       "      <td>Tu ne peux pas abandonner comme ça.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137052</th>\n",
       "      <td>Keep it in mind for the next time.</td>\n",
       "      <td>Garde-le en tête pour la prochaine fois !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53497</th>\n",
       "      <td>We will not surrender.</td>\n",
       "      <td>Nous ne nous rendrons pas.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183066</th>\n",
       "      <td>Tom told me he didn't take French in high school.</td>\n",
       "      <td>Tom m'a dit qu'il n'avait pas pris français au...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "42967                               I saw you doing that.   \n",
       "99991                        You can't give up like this.   \n",
       "137052                 Keep it in mind for the next time.   \n",
       "53497                              We will not surrender.   \n",
       "183066  Tom told me he didn't take French in high school.   \n",
       "\n",
       "                                                      fra  \\\n",
       "42967                           Je vous ai vu faire cela.   \n",
       "99991                 Tu ne peux pas abandonner comme ça.   \n",
       "137052          Garde-le en tête pour la prochaine fois !   \n",
       "53497                          Nous ne nous rendrons pas.   \n",
       "183066  Tom m'a dit qu'il n'avait pas pris français au...   \n",
       "\n",
       "                                                       cc  \n",
       "42967   CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "99991   CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "137052  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "53497   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "183066  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a5198bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20645</th>\n",
       "      <td>The well ran dry.</td>\n",
       "      <td>Le puits s'est asséché.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8275</th>\n",
       "      <td>May I go home?</td>\n",
       "      <td>Puis-je aller chez moi ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6064</th>\n",
       "      <td>Tom followed.</td>\n",
       "      <td>Tom a suivi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6807</th>\n",
       "      <td>Do you get it?</td>\n",
       "      <td>Pigez-vous ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7486</th>\n",
       "      <td>I need a coat.</td>\n",
       "      <td>Il me faut un manteau.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     eng                       fra\n",
       "20645  The well ran dry.   Le puits s'est asséché.\n",
       "8275      May I go home?  Puis-je aller chez moi ?\n",
       "6064       Tom followed.              Tom a suivi.\n",
       "6807      Do you get it?              Pigez-vous ?\n",
       "7486      I need a coat.    Il me faut un manteau."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:33000] # 3.3만개의 샘플\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf4feaf",
   "metadata": {},
   "source": [
    "\n",
    "정제, 정규화, 전처리(영어, 프랑스어 둘다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79aab91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28058</th>\n",
       "      <td>You're productive.</td>\n",
       "      <td>Vous êtes productifs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9170</th>\n",
       "      <td>We were bored.</td>\n",
       "      <td>Nous nous ennuyions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25347</th>\n",
       "      <td>Let's take a vote.</td>\n",
       "      <td>Votons.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25707</th>\n",
       "      <td>She looked lonely.</td>\n",
       "      <td>Elle avait l'air seule.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11856</th>\n",
       "      <td>The birds sang.</td>\n",
       "      <td>Les oiseaux chantaient.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20969</th>\n",
       "      <td>Tom clearly lied.</td>\n",
       "      <td>Tom a manifestement menti.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      eng                         fra\n",
       "28058  You're productive.       Vous êtes productifs.\n",
       "9170       We were bored.        Nous nous ennuyions.\n",
       "25347  Let's take a vote.                     Votons.\n",
       "25707  She looked lonely.     Elle avait l'air seule.\n",
       "11856     The birds sang.     Les oiseaux chantaient.\n",
       "20969   Tom clearly lied.  Tom a manifestement menti."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad41075",
   "metadata": {},
   "source": [
    "전처리 - 구두점 나누기, 소문자로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d049d9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21858</th>\n",
       "      <td>what will you do ?</td>\n",
       "      <td>que feras tu ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>tom was upset .</td>\n",
       "      <td>tom tait contrari .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25149</th>\n",
       "      <td>it s all nonsense .</td>\n",
       "      <td>rien n a de sens .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14220</th>\n",
       "      <td>i know it hurts .</td>\n",
       "      <td>je sais que a fait mal .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8522</th>\n",
       "      <td>take a shower .</td>\n",
       "      <td>va prendre une douche !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng                        fra\n",
       "21858   what will you do ?             que feras tu ? \n",
       "8998       tom was upset .        tom tait contrari . \n",
       "25149  it s all nonsense .         rien n a de sens . \n",
       "14220    i know it hurts .   je sais que a fait mal . \n",
       "8522       take a shower .    va prendre une douche ! "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence) :\n",
    "    \n",
    "    # 전처리 부분\n",
    "\n",
    "    # 구두점을 단어와 분리를 시켜본다\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sentence)\n",
    "    sentence = re.sub(r\"\\s+\", r\" \", sentence)\n",
    "#     sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    # 모두 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "lines.eng = lines.eng.apply(lambda x : preprocess_sentence(x))\n",
    "lines.fra = lines.fra.apply(lambda x : preprocess_sentence(x))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11e8f4",
   "metadata": {},
   "source": [
    "시작토큰과 종료토큰 추가하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a7f9e",
   "metadata": {},
   "source": [
    "프랑스어에만 추가한다. (디코더 언어) decoder_input에는 시작태그만, decoder_target에는 종료태그만 남아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f4200e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 :  33000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27620</th>\n",
       "      <td>who s your father ?</td>\n",
       "      <td>&lt;sos&gt; lequel est votre p re ?  &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16182</th>\n",
       "      <td>tom is a muslim .</td>\n",
       "      <td>&lt;sos&gt; tom est musulman .  &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13261</th>\n",
       "      <td>can you hear us ?</td>\n",
       "      <td>&lt;sos&gt; parvenez vous nous entendre ?  &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27986</th>\n",
       "      <td>you re a survivor .</td>\n",
       "      <td>&lt;sos&gt; tu es un survivant .  &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7831</th>\n",
       "      <td>i m listening .</td>\n",
       "      <td>&lt;sos&gt; j coute .  &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng                                         fra\n",
       "27620  who s your father ?         <sos> lequel est votre p re ?  <eos>\n",
       "16182    tom is a muslim .              <sos> tom est musulman .  <eos>\n",
       "13261    can you hear us ?   <sos> parvenez vous nous entendre ?  <eos>\n",
       "27986  you re a survivor .            <sos> tu es un survivant .  <eos>\n",
       "7831       i m listening .                       <sos> j coute .  <eos>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '<sos>'\n",
    "eos_token = '<eos>'\n",
    "lines.fra = lines.fra.apply(lambda x : sos_token + ' ' + x + ' ' + eos_token)\n",
    "print('전체 샘플의 수 : ', len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d350ce4d",
   "metadata": {},
   "source": [
    "단어 단위 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c7400",
   "metadata": {},
   "source": [
    "Tokenizer의 인자 중 char_level은 default값으로 False이다, 이 인자를 True로 사용한다면 글자 단위의 토큰화를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7c56f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[28, 1], [28, 1], [28, 1]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(filters=\"\", lower=False)            # 토큰화 수행 : 문자 단위 X\n",
    "eng_tokenizer.fit_on_texts(lines.eng)   # 33000개의 데이터 각 행을 토큰화\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)   # 단어를 숫자값 인덱스로 변환\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b856acde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 75, 8, 2], [1, 365, 3, 2], [1, 28, 512, 8, 2]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(filters=\"\", lower=False)\n",
    "fra_tokenizer.fit_on_texts(lines.fra)\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d15b5",
   "metadata": {},
   "source": [
    "단어장의 사이즈를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8eb9ae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 :  4671\n",
      "프랑스어 단어장의 크기:  7454\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print(\"영어 단어장의 크기 : \", eng_vocab_size)\n",
    "print(\"프랑스어 단어장의 크기: \", fra_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2cd84",
   "metadata": {},
   "source": [
    "패딩추가를 위해서 최대 길이를 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee50a0",
   "metadata": {},
   "source": [
    "디코더의 데이터 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc8f5c7",
   "metadata": {},
   "source": [
    "디코더의 입력에는 <eos> 토큰이 필요없고, 디코더의 출력과 비교할 시퀀스는 <sos>가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be6807d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text]\n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f148e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 75, 8], [1, 365, 3], [1, 28, 512, 8]]\n",
      "[[75, 8, 2], [365, 3, 2], [28, 512, 8, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a81249d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c69a329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4671\n",
      "프랑스어 단어장의 크기 : 7454\n",
      "영어 시퀀스의 최대 길이 :  8\n",
      "프랑스어 시퀀스의 최대 길이 :  17\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이 : ', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이 : ', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3389b820",
   "metadata": {},
   "source": [
    "패딩 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22624ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 17)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 17)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen=max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60c8f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_to_index = eng_tokenizer.word_index\n",
    "index_to_eng = eng_tokenizer.index_word\n",
    "\n",
    "fra_to_index = fra_tokenizer.word_index\n",
    "index_to_fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9228872",
   "metadata": {},
   "source": [
    "데이터셋 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58789ce",
   "metadata": {},
   "source": [
    "데이터를 나누기 전에 먼저 한번 섞어준 후에 Training 3만개, Test 3천개로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe103136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 17)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 17)\n"
     ]
    }
   ],
   "source": [
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80925c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18789  4004  1104 ... 15708 28521 12618]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "987e951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6dc1aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:30000]\n",
    "decoder_input_train = decoder_input[:30000]\n",
    "decoder_target_train = decoder_target[:30000]\n",
    "\n",
    "encoder_input_test = encoder_input[30000:]\n",
    "decoder_input_test = decoder_input[30000:]\n",
    "decoder_target_test = decoder_target[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4db3485e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 8)\n",
      "(30000, 17)\n",
      "(30000, 17)\n",
      "(3000, 8)\n",
      "(3000, 17)\n",
      "(3000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_train.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_target_train.shape)\n",
    "print(encoder_input_test.shape)\n",
    "print(decoder_input_test.shape)\n",
    "print(decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c9fea",
   "metadata": {},
   "source": [
    "모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e2a05",
   "metadata": {},
   "source": [
    "필요한 라이브러리 import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "34d322a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f6d2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM의 출력 차원\n",
    "latent_dim = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c4910",
   "metadata": {},
   "source": [
    "인코더 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38a476",
   "metadata": {},
   "source": [
    "Masking은 패딩 토큰의 숫자 0의 경우에는 연산을 제외하는 역할을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b85736f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(eng_vocab_size, latent_dim)(encoder_inputs) # 임베딩층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)  # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)  # 상태값 리턴\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)  # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c]  # 인코더의 은닉 상태외 셀 상태를 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b02db",
   "metadata": {},
   "source": [
    "디코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d96d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "dec_emb_layer = Embedding(fra_vocab_size, latent_dim)  # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs)  # 패딩 0은 언제나 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequence는 True\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태 (initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점에 결과에 대해 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a821a9ff",
   "metadata": {},
   "source": [
    "학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ae03d8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 32)     149472      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 32)     238528      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 32)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None, 32)     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 32), (None,  8320        masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 32), ( 8320        masking_2[0][0]                  \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 7454)   245982      lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 650,622\n",
      "Trainable params: 650,622\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3028fdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 14s 33ms/step - loss: 3.8018 - acc: 0.6249 - val_loss: 2.0191 - val_acc: 0.6303\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.7756 - acc: 0.6972 - val_loss: 1.6063 - val_acc: 0.7506\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.5339 - acc: 0.7539 - val_loss: 1.4819 - val_acc: 0.7641\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.4285 - acc: 0.7661 - val_loss: 1.4029 - val_acc: 0.7731\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.3582 - acc: 0.7774 - val_loss: 1.3486 - val_acc: 0.7851\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.3049 - acc: 0.7886 - val_loss: 1.3050 - val_acc: 0.7945\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.2600 - acc: 0.7959 - val_loss: 1.2701 - val_acc: 0.7989\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 1.2214 - acc: 0.8016 - val_loss: 1.2339 - val_acc: 0.8043\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.1882 - acc: 0.8062 - val_loss: 1.2093 - val_acc: 0.8078\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.1580 - acc: 0.8104 - val_loss: 1.1833 - val_acc: 0.8123\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.1309 - acc: 0.8143 - val_loss: 1.1650 - val_acc: 0.8144\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.1059 - acc: 0.8182 - val_loss: 1.1436 - val_acc: 0.8178\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.0825 - acc: 0.8218 - val_loss: 1.1241 - val_acc: 0.8214\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.0615 - acc: 0.8247 - val_loss: 1.1080 - val_acc: 0.8240\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.0421 - acc: 0.8275 - val_loss: 1.0959 - val_acc: 0.8252\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.0243 - acc: 0.8298 - val_loss: 1.0878 - val_acc: 0.8264\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 1.0078 - acc: 0.8318 - val_loss: 1.0710 - val_acc: 0.8288\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.9929 - acc: 0.8337 - val_loss: 1.0561 - val_acc: 0.8300\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.9783 - acc: 0.8358 - val_loss: 1.0480 - val_acc: 0.8316\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.9647 - acc: 0.8375 - val_loss: 1.0371 - val_acc: 0.8323\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.9518 - acc: 0.8392 - val_loss: 1.0308 - val_acc: 0.8331\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.9389 - acc: 0.8409 - val_loss: 1.0199 - val_acc: 0.8345\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.9263 - acc: 0.8425 - val_loss: 1.0113 - val_acc: 0.8355\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.9142 - acc: 0.8439 - val_loss: 1.0035 - val_acc: 0.8360\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.9029 - acc: 0.8452 - val_loss: 0.9950 - val_acc: 0.8378\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.8922 - acc: 0.8463 - val_loss: 0.9916 - val_acc: 0.8379\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.8823 - acc: 0.8479 - val_loss: 0.9850 - val_acc: 0.8383\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.8728 - acc: 0.8490 - val_loss: 0.9757 - val_acc: 0.8401\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.8635 - acc: 0.8501 - val_loss: 0.9715 - val_acc: 0.8405\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.8544 - acc: 0.8514 - val_loss: 0.9676 - val_acc: 0.8412\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.8460 - acc: 0.8526 - val_loss: 0.9594 - val_acc: 0.8431\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.8380 - acc: 0.8536 - val_loss: 0.9582 - val_acc: 0.8426\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.8304 - acc: 0.8549 - val_loss: 0.9533 - val_acc: 0.8426\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.8236 - acc: 0.8559 - val_loss: 0.9487 - val_acc: 0.8440\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.8174 - acc: 0.8569 - val_loss: 0.9481 - val_acc: 0.8448\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.8116 - acc: 0.8579 - val_loss: 0.9453 - val_acc: 0.8450\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.8063 - acc: 0.8589 - val_loss: 0.9407 - val_acc: 0.8459\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.8012 - acc: 0.8602 - val_loss: 0.9389 - val_acc: 0.8462\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.7960 - acc: 0.8612 - val_loss: 0.9355 - val_acc: 0.8474\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.7916 - acc: 0.8620 - val_loss: 0.9335 - val_acc: 0.8475\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.7866 - acc: 0.8628 - val_loss: 0.9323 - val_acc: 0.8474\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.7818 - acc: 0.8639 - val_loss: 0.9299 - val_acc: 0.8483\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.7771 - acc: 0.8645 - val_loss: 0.9325 - val_acc: 0.8473\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.7718 - acc: 0.8658 - val_loss: 0.9254 - val_acc: 0.8492\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.7670 - acc: 0.8665 - val_loss: 0.9226 - val_acc: 0.8493\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.7611 - acc: 0.8674 - val_loss: 0.9211 - val_acc: 0.8499\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.7552 - acc: 0.8681 - val_loss: 0.9167 - val_acc: 0.8506\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.7498 - acc: 0.8689 - val_loss: 0.9180 - val_acc: 0.8492\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.7445 - acc: 0.8698 - val_loss: 0.9116 - val_acc: 0.8507\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.7399 - acc: 0.8705 - val_loss: 0.9128 - val_acc: 0.8506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f58f90cc880>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 128, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd7d82f",
   "metadata": {},
   "source": [
    "모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726d029",
   "metadata": {},
   "source": [
    "인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "370da852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 32)          149472    \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                [(None, 32), (None, 32),  8320      \n",
      "=================================================================\n",
      "Total params: 157,792\n",
      "Trainable params: 157,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd0f884",
   "metadata": {},
   "source": [
    "디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34934f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 설계\n",
    "# 이전 시점의 상태롤 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# train 때 사용했던 임베딩 층을 재사용..\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단여 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a18509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 정의\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8d082bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 정수 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = fra_to_index['<sos>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_fra[sampled_token_index]\n",
    "\n",
    "         # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129869f",
   "metadata": {},
   "source": [
    "결과 확인을 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "71e475b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2eng(input_seq):\n",
    "    temp = ''\n",
    "    for i in input_seq :\n",
    "        if(i!=0):\n",
    "            temp = temp + index_to_eng[i] + ' '\n",
    "    return temp\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2fra(input_seq):\n",
    "    temp = ''\n",
    "    for i in input_seq:\n",
    "        if ((i!=0 and i!=fra_to_index['<sos>']) and i!=fra_to_index['<eos>']):\n",
    "            temp = temp + index_to_fra[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcc794",
   "metadata": {},
   "source": [
    "훈련 데이터에 대해서 임의로 선택한 인덱스의 샘플 결과를 출력\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8e363fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  let s be objective . \n",
      "번역문 :  soyons objectives . \n",
      "예측문 :   soyez nous . \n",
      "\n",
      "\n",
      "원문 :  relax . \n",
      "번역문 :  d tends toi . \n",
      "예측문 :   d je vous prie ! \n",
      "\n",
      "\n",
      "원문 :  was tom hurt ? \n",
      "번역문 :  tom a t il t bless ? \n",
      "예측문 :   tom tait il ? \n",
      "\n",
      "\n",
      "원문 :  i feel sick . \n",
      "번역문 :  je me sens mal . \n",
      "예측문 :   je me sens en train de cie . \n",
      "\n",
      "\n",
      "원문 :  we broke up . \n",
      "번역문 :  nous nous sommes s par s . \n",
      "예측문 :   nous nous sommes deux . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 600, 2005]:\n",
    "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    print(\"원문 : \", seq2eng(encoder_input_train[seq_index]))\n",
    "    print(\"번역문 : \", seq2fra(decoder_input_train[seq_index]))\n",
    "    print(\"예측문 : \", decoded_sentence[:-5])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abbdaed",
   "metadata": {},
   "source": [
    "정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2741a93",
   "metadata": {},
   "source": [
    "validation loss가 안정적으로 떨어지면서 0.9354까지 떨어지면서 학습 중간에 Overfitting은 관찰되지 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01397eb",
   "metadata": {},
   "source": [
    "또 테스트용 디코더 input을 2개로 받아오면서 이전에 학습할때 사용한 디코더와는 구조가 다르다. 원래 들어오는 input + input의 상태\n",
    "와 이전 시점에서 나오는 outputs과 그 상태에서 다르다.\n",
    "영어와 프랑스어 번역을 진행해 보았는데 이렇게 스페인어도 도전해보고 싶다. 간단한 사전, 회화 어플리케이션이 이렇게 만들어지는 것 같아 보이는데 더 나아가 장문의 텍스트를 번역하려면 데이터 양이 방대할 것이라는 생각이 든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3937acd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
